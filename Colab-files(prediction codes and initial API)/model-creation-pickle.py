# -*- coding: utf-8 -*-
"""optum.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1czYzIZObK2MIaalTGvqeje68tVssxThG
"""

#Import Required Packages

import pandas as pd
import numpy as np
import matplotlib as plt
from sklearn.model_selection import train_test_split 
from xgboost import plot_importance
import xgboost
from sklearn.metrics import mean_squared_error as MSE 
from sklearn.svm import SVR
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix 
from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, ExtraTreesRegressor, GradientBoostingRegressor
from sklearn.preprocessing import StandardScaler, Normalizer, RobustScaler

#Read Observ.csv and Analysing the file

obsrv=pd.read_csv("observations.csv")
obsrv=obsrv[["DATE","PATIENT","DESCRIPTION","VALUE"]]
obsrv["INDEX"]=pd.Series(range(0,len(obsrv),1))
obsrv["DATE"]=pd.to_datetime(obsrv["DATE"].str[:10])
body_features=obsrv.DESCRIPTION.unique()
freq=obsrv["DESCRIPTION"].value_counts()
print(freq)
for i in range(len(obsrv)):
  if obsrv.DESCRIPTION.iloc[i] not in body_features or freq[obsrv.DESCRIPTION.iloc[i]]<100:
    obsrv.DESCRIPTION.iloc[i]="aa"
body_features=obsrv["DESCRIPTION"].unique()
print(obsrv["DESCRIPTION"].value_counts())
obsrv=obsrv.pivot_table(index=["INDEX","DATE","PATIENT"],columns="DESCRIPTION",aggfunc= lambda x: x)

#Read The Patients Data

patients=pd.read_csv("patients.csv")
patients=patients[["Id","BIRTHDATE","MARITAL","RACE","ETHNICITY","GENDER","LAT","LON"]]

#Drop the na row of Reasoncode
d['Body Weight'].isna().sum()
d=d.dropna(subset=["REASONCODE"])

#Read The medication data and Clean the data

medi=pd.read_csv("medications.csv")
medi=medi[['PATIENT','START','TOTALCOST','REASONCODE',"PAYER_COVERAGE","REASONDESCRIPTION"]]
medi.rename(columns = {'PATIENT':'Id'}, inplace = True) 
data=pd.merge(medi,patients,how='left',on='Id')
data["START"]=pd.to_datetime(data["START"].str[:10])
data["BIRTHDATE"]=pd.to_datetime(data["BIRTHDATE"])
data["AGE"]=(data["START"]-data["BIRTHDATE"]).dt.days/365
data["REASONCODE"].unique()

#Predicting the total healthcare cost incureed
#Prepare the data ready for training

X=d[["AGE","LAT","LON","GENDER","ETHNICITY","RACE","MARITAL","REASONCODE"]]
Y=d["TOTALCOST"]
print(X.shape)
print(Y.shape)
gender=pd.get_dummies(X['GENDER'],prefix='gender')
ethnicity=pd.get_dummies(X['ETHNICITY'],prefix='ethnicity')
race=pd.get_dummies(X['RACE'],prefix='race')
marital=pd.get_dummies(X['MARITAL'],prefix='marital')
reasoncode=pd.get_dummies(X["REASONCODE"],prefix='reasoncode')

mu_Age=X["AGE"].mean()
std_Age=np.std(X["AGE"])
#X["AGE"]=(X["AGE"]-mu_Age)/std_Age

mu_lat=X["LAT"].mean()
std_lat=np.std(X["LAT"])
#X["LAT"]=(X["LAT"]-mu_lat)/std_lat

mu_lon=X["LON"].mean()
std_lon=np.std(X["LON"])
#X["LON"]=(X["LON"]-mu_lon)/std_lon

X=pd.concat([X["AGE"],X["LAT"],X["LON"],gender,marital,ethnicity,race,reasoncode],axis=1)
train_X,test_X,train_Y,test_Y=train_test_split(X,Y,test_size=0.3,random_state=123)

#Substitute the  NA values of Marital status column
for i in range(len(d)):
  d.MARITAL.iloc[i]=str(d.MARITAL.iloc[i])[:1]
  if d.MARITAL.iloc[i]=='n':
    if d.AGE.iloc[i]<25:
      d.MARITAL.iloc[i]='S'
    else:
      d.MARITAL.iloc[i]='M'

#Create a SVR model 
regr = make_pipeline(StandardScaler(), SVR(kernel='poly',degree=9,C=1.0, epsilon=0.1)) 

#Train the model
regr.fit(train_X,train_Y)

#Test on test_data and check the error value
y_pred = regr.predict(test_X)
np.sqrt(MSE(test_Y, y_pred))

#Download the model
import pickle
with open('predict2.pkl', 'wb') as fid:
     pickle.dump(regr, fid)
files.download(predict2.pkl)

#Predicting the cost incurred by the Payer
#Getting the data ready for trainging

X=d[["AGE","LAT","LON","GENDER","ETHNICITY","RACE","MARITAL","REASONCODE"]]
Y=d["PAYER_COVERAGE"]
print(X.shape)
print(Y.shape)
gender=pd.get_dummies(X['GENDER'],prefix='gender')
ethnicity=pd.get_dummies(X['ETHNICITY'],prefix='ethnicity')
race=pd.get_dummies(X['RACE'],prefix='race')
marital=pd.get_dummies(X['MARITAL'],prefix='marital')
reasoncode=pd.get_dummies(X["REASONCODE"],prefix='reasoncode')

mu_Age=X["AGE"].mean()
std_Age=np.std(X["AGE"])
#X["AGE"]=(X["AGE"]-mu_Age)/std_Age

mu_lat=X["LAT"].mean()
std_lat=np.std(X["LAT"])
#X["LAT"]=(X["LAT"]-mu_lat)/std_lat

mu_lon=X["LON"].mean()
std_lon=np.std(X["LON"])
#X["LON"]=(X["LON"]-mu_lon)/std_lon

X=pd.concat([X["AGE"],X["LAT"],X["LON"],gender,marital,ethnicity,race,reasoncode],axis=1)
train_X,test_X,train_Y,test_Y=train_test_split(X,Y,test_size=0.3,random_state=123)

from sklearn.datasets import load_diabetes
from sklearn.linear_model import RidgeCV
from sklearn.svm import LinearSVR
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import StackingRegressor

#Create an SVR and a RidgeCV model and stack them up to get more accuracy
estimators = [('lr', RidgeCV()),('svr', LinearSVR(random_state=42))]

#Train the model on training data
reg = StackingRegressor(estimators=estimators,final_estimator=RandomForestRegressor(n_estimators=20,random_state=42))
reg.fit(train_X,train_Y)

Test the model 
y_pred = reg.predict(test_X)
np.sqrt(MSE(test_Y, y_pred))

#Read the careplan data and clean the data

careplan=pd.read_csv("careplans.csv")
careplan=careplan.dropna(subset=["REASONCODE"])
patients.rename(columns = {'Id':'PATIENT'}, inplace = True) 
data_1=pd.merge(careplan,patients,how='left',on='PATIENT')
data_1["START"]=pd.to_datetime(data_1["START"].str[:10])
data_1["BIRTHDATE"]=pd.to_datetime(data_1["BIRTHDATE"])
data_1["AGE"]=(data_1["START"]-data_1["BIRTHDATE"]).dt.days/365
data_1["REASONCODE"].unique()

#Predicting the best therapy for a given diesease
#Preparing the data for training and testing

X=data_1[["AGE","LAT","LON","GENDER","ETHNICITY","RACE","MARITAL","REASONCODE"]]
Y=data_1["CODE"]
print(X.shape)
print(Y.shape)
gender=pd.get_dummies(X['GENDER'],prefix='gender')
ethnicity=pd.get_dummies(X['ETHNICITY'],prefix='ethnicity')
race=pd.get_dummies(X['RACE'],prefix='race')
marital=pd.get_dummies(X['MARITAL'],prefix='marital')
reasoncode=pd.get_dummies(X["REASONCODE"],prefix='reasoncode')

mu_Age=X["AGE"].mean()
std_Age=np.std(X["AGE"])
#X["AGE"]=(X["AGE"]-mu_Age)/std_Age

mu_lat=X["LAT"].mean()
std_lat=np.std(X["LAT"])
#X["LAT"]=(X["LAT"]-mu_lat)/std_lat

mu_lon=X["LON"].mean()
std_lon=np.std(X["LON"])
#X["LON"]=(X["LON"]-mu_lon)/std_lon

X=pd.concat([X["AGE"],X["LAT"],X["LON"],gender,marital,ethnicity,race,reasoncode],axis=1)
train_X,test_X,train_Y,test_Y=train_test_split(X,Y,test_size=0.3,random_state=123)

from sklearn.metrics import log_loss
from sklearn.metrics import accuracy_score

#Create a RandomClassifier model.
clf=RandomForestClassifier(max_depth=35, random_state=0)

#Train the model
clf.fit(train_X,train_Y)

Test the mofel from test data
pred_y=clf.predict(test_X)
probs=clf.predict_proba(test_X)
cm = confusion_matrix(test_Y,pred_y)
print(pred_y.shape)
print(test_X.shape)
print(probs.shape)
accuracy_score(test_Y, pred_y)

import pickle
with open('predict_careplan.pkl', 'wb') as fid:
     pickle.dump(clf, fid)

X=data_1[["AGE","LAT","LON","GENDER","ETHNICITY","RACE","MARITAL","REASONCODE"]]
Y=data_1["CODE"]
print(X.shape)
print(Y.shape)
gender=pd.get_dummies(X['GENDER'],prefix='gender')
ethnicity=pd.get_dummies(X['ETHNICITY'],prefix='ethnicity')
race=pd.get_dummies(X['RACE'],prefix='race')
marital=pd.get_dummies(X['MARITAL'],prefix='marital')
reasoncode=pd.get_dummies(X["REASONCODE"],prefix='reasoncode')

mu_Age=X["AGE"].mean()
std_Age=np.std(X["AGE"])
#X["AGE"]=(X["AGE"]-mu_Age)/std_Age

mu_lat=X["LAT"].mean()
std_lat=np.std(X["LAT"])
#X["LAT"]=(X["LAT"]-mu_lat)/std_lat

mu_lon=X["LON"].mean()
std_lon=np.std(X["LON"])
#X["LON"]=(X["LON"]-mu_lon)/std_lon

X=pd.concat([X["AGE"],X["LAT"],X["LON"],gender,marital,ethnicity,race,reasoncode],axis=1)
train_X,test_X,train_Y,test_Y=train_test_split(X,Y,test_size=0.3,random_state=123)

from sklearn.metrics import log_loss
from sklearn.metrics import accuracy_score
clf=RandomForestClassifier(max_depth=35, random_state=0)
clf.fit(train_X,train_Y)
pred_y=clf.predict(test_X)
probs=clf.predict_proba(test_X)
cm = confusion_matrix(test_Y,pred_y)
print(pred_y.shape)
print(test_X.shape)
print(probs.shape)
accuracy_score(test_Y, pred_y)